name: SQL Flow Builder
description: >
  Builds end-to-end data pipeline job JSON and SQL from source-to-target mapping CSVs.
triggers:
  - pattern: "generate sql flow"
  - pattern: "build job for"
tasks:
  - run: python build_sql_job.py ${filePath}
  - summarize: "Review the generated SQL and JSON and confirm output paths."


.copilot/agents/sql_flow_builder.yaml:


# .copilot/agents/sql_flow_builder.yaml
name: SQL Flow Builder
description: |
  Build complete SQL + Job JSON pipelines from source-to-target mapping CSVs.
  Supports business rule parsing, CTE-based transformations, and Copilot-driven generation.

# Copilot will respond to these commands in chat or terminal
triggers:
  - pattern: "generate sql flow"
    description: "Generate full SQL + job JSON from a mapping CSV."
  - pattern: "build job for"
    description: "Build SQL + job JSON for the given mapping CSV file."
  - pattern: "create pipeline for"
    description: "Create SQL transformation pipeline for provided mapping CSV."

# Actions Copilot executes when trigger fires
steps:
  - ask: |
      Please provide the CSV file path (e.g., `mappings/source_target_mapping_clean_v9_fixed.csv`)
  - run: |
      python build_sql_job.py ${filePath} --outdir generated_jobs
  - summarize: |
      âœ… Generated SQL flow for ${filePath}
      - SQL + JSON files saved in: `generated_jobs/<target>_job/`
      - SQL CTE pipeline: `<target>_pipeline.sql`
      - Job definition: `<target>_job.json`
      You can open the generated folder to inspect the logic.

permissions:
  filesystem: read-write
  terminal: allowed


@SQL Flow Builder generate sql flow
mappings/source_target_mapping_clean_v9_fixed.csv)

mappings/source_target_mapping_clean_v9_fixed.csv

python build_sql_job.py mappings/source_target_mapping_clean_v9_fixed.csv --outdir generated_jobs


# .copilot/agents/sql_flow_builder.yaml
name: SQL Flow Builder
description: |
  Automatically generate SQL + Job JSON pipelines from source-to-target mapping CSVs.
  Supports CTE-based transformations, business-rule parsing, and full Copilot integration.

# Triggers that launch the Agent
triggers:
  - pattern: "generate sql flow"
    description: "Generate SQL + job JSON from a mapping CSV."
  - pattern: "build job for"
    description: "Build SQL + job JSON for the given mapping CSV file."
  - pattern: "create pipeline for"
    description: "Create a SQL transformation pipeline for the given CSV file."

# Steps the Agent executes
steps:
  # Step 1: Ask for the CSV file path if not provided
  - ask: |
      Please provide the full CSV file path (e.g., `mappings/source_target_mapping_clean_v9_fixed.csv`)
  
  # Step 2: Detect target name dynamically from CSV file name
  - run: |
      filename=$(basename ${filePath})
      target_name="${filename%.*}"
      echo "Detected target name: ${target_name}"

  # Step 3: Run the generator
  - run: |
      python build_sql_job.py ${filePath} --outdir generated_jobs

  # Step 4: Summarize the results dynamically
  - summarize: |
      âœ… Generated SQL flow for **${filePath}**
      **Target detected:** ${target_name}
      
      ðŸ“‚ Output directory: `generated_jobs/${target_name}_job/`
      - SQL pipeline: `${target_name}_pipeline.sql`
      - Job JSON: `${target_name}_job.json`
      
      ðŸ§  Summary:
      - Uses CTE-style transformations
      - Full CASE/WHEN/END logic retained
      - Business Rules converted to auditable WHERE clauses with inline comments

      You can open the generated SQL file to inspect logic.

  # Step 5: Open the generated SQL file in VS Code automatically
  - run: |
      code generated_jobs/${target_name}_job/${target_name}_pipeline.sql

permissions:
  filesystem: read-write
  terminal: allowed
  code_editor: open

steps:
  - run: |
      python build_sql_job.py ${filePath} --outdir generated_jobs --infer-job-type
  - summarize: |
      âœ… Generated job type: ${jobType}
      Modules created: ${modulesList}


